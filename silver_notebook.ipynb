{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b12588b-5321-42ef-b012-571a43759743",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='abfss://gold@stockagedataengineering.dfs.core.windows.net/Consultation.csv', name='Consultation.csv', size=14466626, modificationTime=1746460728000),\n",
       " FileInfo(path='abfss://gold@stockagedataengineering.dfs.core.windows.net/Date.csv', name='Date.csv', size=1908623, modificationTime=1746460724000)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mount ADLS Gen2\n",
    "# Required each time the cluster is restarted which should be only on the first notebook as they run in order\n",
    "\n",
    "tiers = [\"bronze\", \"silver\", \"gold\"]\n",
    "adls_paths = {tier: f\"abfss://{tier}@stockagedataengineering.dfs.core.windows.net/\" for tier in tiers}\n",
    "\n",
    "# Accessing paths\n",
    "bronze_adls = adls_paths[\"bronze\"]\n",
    "silver_adls = adls_paths[\"silver\"]\n",
    "gold_adls = adls_paths[\"gold\"] \n",
    "\n",
    "dbutils.fs.ls(bronze_adls)\n",
    "dbutils.fs.ls(silver_adls)\n",
    "dbutils.fs.ls(gold_adls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd6e71c8-0c9d-4cca-872f-8149699dd684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tous les fichiers transformés et enregistrés avec noms traduits en français.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "\n",
    "# Dossiers Bronze et Silver\n",
    "bronze_path = bronze_adls\n",
    "silver_path = silver_adls\n",
    "\n",
    "# Dictionnaire des noms traduits\n",
    "translations = {\n",
    "    \n",
    "    \"Allergy\": \"Allergie\",\n",
    "    \"Date\": \"Date\",\n",
    "    \"Disease\": \"Maladie\",\n",
    "    \"Doctor\": \"Medecin\",\n",
    "    \"Patient\": \"Patient\",\n",
    "    \"Room\": \"Chambre\",\n",
    "    \"Treatment\": \"Traitement\",\n",
    "    \"TreatmentCost\": \"CoutTraitement\",\n",
    "    \"Cost\": \"Cout\",\n",
    "    \"Encounter\": \"Consultation\",\n",
    "    \"Vitals\": \"SignesVitaux\",\n",
    "    \"Patient_Allergy\": \"Patient_Allergie\",\n",
    "    \"BridgeEncounterDoctor\": \"Docteur_consultation\"\n",
    "    # Ajoute d'autres si besoin\n",
    "}\n",
    "\n",
    "# Liste tous les fichiers CSV du dossier Bronze\n",
    "csv_files = [f.path for f in dbutils.fs.ls(bronze_path) if f.path.endswith(\".csv\")]\n",
    "\n",
    "for file_path in csv_files:\n",
    "    # Lire le fichier\n",
    "    df = spark.read.option(\"header\", \"true\").csv(file_path)\n",
    "    df = df.dropDuplicates()\n",
    "\n",
    "    # Remplir les valeurs manquantes\n",
    "    for column, dtype in df.dtypes:\n",
    "        if dtype in [\"double\", \"int\", \"float\", \"bigint\"]:\n",
    "            median = df.approxQuantile(column, [0.5], 0.001)[0]\n",
    "            df = df.withColumn(column, when(col(column).isNull(), median).otherwise(col(column)))\n",
    "        else:\n",
    "            mode_df = df.groupBy(column).count().orderBy(F.desc(\"count\")).first()\n",
    "            if mode_df:\n",
    "                mode_value = mode_df[0]\n",
    "                df = df.withColumn(column, when(col(column).isNull(), mode_value).otherwise(col(column)))\n",
    "\n",
    "    # Nettoyer le nom du fichier\n",
    "    filename = os.path.basename(file_path)\n",
    "    base_name = filename.replace(\".csv\", \"\")\n",
    "    \n",
    "    # Supprimer les préfixes \"Dim_\", \"Fact_\", etc.\n",
    "    for prefix in [\"Dim_\", \"Fact_\", \"Dim\", \"Fact\"]:\n",
    "        if base_name.startswith(prefix):\n",
    "            base_name = base_name[len(prefix):]\n",
    "\n",
    "    # Récupérer le nom traduit, sinon garder le nom sans préfixe\n",
    "    translated_name = translations.get(base_name, base_name)\n",
    "    final_filename = translated_name + \".csv\"\n",
    "\n",
    "    # Écriture dans un dossier temporaire\n",
    "    tmp_output_path = silver_path + \"_tmp_\" + translated_name + \"/\"\n",
    "    (df.coalesce(1)\n",
    "       .write\n",
    "       .mode(\"overwrite\")\n",
    "       .option(\"header\", \"true\")\n",
    "       .csv(tmp_output_path))\n",
    "\n",
    "    # Trouver le fichier part-xxxxx.csv\n",
    "    tmp_files = dbutils.fs.ls(tmp_output_path)\n",
    "    part_file = [f.path for f in tmp_files if f.name.startswith(\"part-\")][0]\n",
    "\n",
    "    # Définir le chemin final dans Silver\n",
    "    final_output_path = silver_path + final_filename\n",
    "\n",
    "    # Déplacer et renommer\n",
    "    dbutils.fs.mv(part_file, final_output_path)\n",
    "\n",
    "    # Supprimer les fichiers temporaires\n",
    "    dbutils.fs.rm(tmp_output_path, recurse=True)\n",
    "\n",
    "print(\" Tous les fichiers transformés et enregistrés avec noms traduits en français.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64f90ff8-521d-4927-abb5-5057234d1b52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, count, when\n",
    "# from pyspark.sql import functions as F\n",
    "# import os\n",
    "\n",
    "# # Dossiers Bronze et Silver\n",
    "# bronze_path = bronze_adls\n",
    "# silver_path = silver_adls\n",
    "\n",
    "# # Liste de tous les fichiers CSV du dossier Bronze\n",
    "# csv_files = [f.path for f in dbutils.fs.ls(bronze_path) if f.path.endswith('.csv')]\n",
    "\n",
    "# for file_path in csv_files:\n",
    "#     # Lire chaque fichier CSV\n",
    "#     df = spark.read.option(\"header\", \"true\").csv(file_path)\n",
    "\n",
    "#     # Supprimer les doublons\n",
    "#     df = df.dropDuplicates()\n",
    "\n",
    "#     # Remplir les valeurs manquantes\n",
    "#     for column, dtype in df.dtypes:\n",
    "#         if dtype in ['double', 'int', 'float', 'bigint']:\n",
    "#             median = df.approxQuantile(column, [0.5], 0.001)[0]\n",
    "#             df = df.withColumn(column, when(col(column).isNull(), median).otherwise(col(column)))\n",
    "#         else:\n",
    "#             mode_df = df.groupBy(column).count().orderBy(F.desc(\"count\")).first()\n",
    "#             if mode_df:\n",
    "#                 mode_value = mode_df[0]\n",
    "#                 df = df.withColumn(column, when(col(column).isNull(), mode_value).otherwise(col(column)))\n",
    "\n",
    "#     # Obtenir le nom de fichier original\n",
    "#     filename = os.path.basename(file_path)\n",
    "\n",
    "#     # Définir un chemin temporaire pour écrire\n",
    "#     tmp_output_path = silver_path + \"_tmp_\" + filename.replace(\".csv\", \"/\")\n",
    "\n",
    "#     # Ecrire le DataFrame en CSV (un seul fichier) dans un dossier temporaire\n",
    "#     (df.coalesce(1)\n",
    "#        .write\n",
    "#        .mode(\"overwrite\")\n",
    "#        .option(\"header\", \"true\")\n",
    "#        .csv(tmp_output_path))\n",
    "\n",
    "#     # Trouver le fichier part-*.csv dans le dossier temporaire\n",
    "#     tmp_files = dbutils.fs.ls(tmp_output_path)\n",
    "#     part_file = [f.path for f in tmp_files if f.name.startswith(\"part-\")][0]\n",
    "\n",
    "#     # Chemin final : fichier sous Silver avec le vrai nom\n",
    "#     final_output_path = silver_path + filename\n",
    "\n",
    "#     # Déplacer et renommer le fichier part-xxxxx.csv vers le chemin final\n",
    "#     dbutils.fs.mv(part_file, final_output_path)\n",
    "\n",
    "#     # Supprimer le dossier temporaire\n",
    "#     dbutils.fs.rm(tmp_output_path, recurse=True)\n",
    "\n",
    "# print(\"✅ Tous les fichiers transformés et enregistrés proprement sans fichiers parasites.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
