{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3d03094-f7d6-47b2-8c24-6da33acd07d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='abfss://gold@stockagedataengineering.dfs.core.windows.net/Consultation.csv', name='Consultation.csv', size=14466626, modificationTime=1746460728000),\n",
       " FileInfo(path='abfss://gold@stockagedataengineering.dfs.core.windows.net/Date.csv', name='Date.csv', size=1908623, modificationTime=1746460724000),\n",
       " FileInfo(path='abfss://gold@stockagedataengineering.dfs.core.windows.net/agregations/', name='agregations/', size=0, modificationTime=1746461915000)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mount ADLS Gen2\n",
    "# Required each time the cluster is restarted which should be only on the first notebook as they run in order\n",
    "\n",
    "tiers = [\"bronze\", \"silver\", \"gold\"]\n",
    "adls_paths = {tier: f\"abfss://{tier}@stockagedataengineering.dfs.core.windows.net/\" for tier in tiers}\n",
    "\n",
    "# Accessing paths\n",
    "bronze_adls = adls_paths[\"bronze\"]\n",
    "silver_adls = adls_paths[\"silver\"]\n",
    "gold_adls = adls_paths[\"gold\"] \n",
    "\n",
    "dbutils.fs.ls(bronze_adls)\n",
    "dbutils.fs.ls(silver_adls)\n",
    "dbutils.fs.ls(gold_adls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fc4b7e4-9df5-46c3-a2a5-8523b5e8ef82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp, date_format\n",
    "import os\n",
    "\n",
    "# === Traitement du fichier Date.csv ===\n",
    "df_date = spark.read.option(\"header\", \"true\").csv(silver_adls + \"Date.csv\")\n",
    "\n",
    "# Conversion et formatage\n",
    "df_date = df_date.withColumn(\"Date\", date_format(to_timestamp(\"Date\"), \"dd/MM/yyyy\"))\n",
    "\n",
    "# Écriture dans un dossier temporaire Gold\n",
    "tmp_path_date = gold_adls + \"_tmp_Date/\"\n",
    "(df_date.coalesce(1)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(tmp_path_date))\n",
    "\n",
    "# Récupérer le fichier part-xxxxx.csv\n",
    "part_file_date = [f.path for f in dbutils.fs.ls(tmp_path_date) if f.name.startswith(\"part-\")][0]\n",
    "final_path_date = gold_adls + \"Date.csv\"\n",
    "dbutils.fs.mv(part_file_date, final_path_date)\n",
    "dbutils.fs.rm(tmp_path_date, recurse=True)\n",
    "\n",
    "\n",
    "# === Traitement du fichier Consultation.csv ===\n",
    "df_consult = spark.read.option(\"header\", \"true\").csv(silver_adls + \"Consultation.csv\")\n",
    "\n",
    "# Conversion et formatage des dates\n",
    "df_consult = (df_consult\n",
    "    .withColumn(\"CheckinDate\", date_format(to_timestamp(\"CheckinDate\"), \"dd/MM/yyyy\"))\n",
    "    .withColumn(\"CheckoutDate\", date_format(to_timestamp(\"CheckoutDate\"), \"dd/MM/yyyy\")))\n",
    "\n",
    "# Écriture dans un dossier temporaire Gold\n",
    "tmp_path_consult = gold_adls + \"_tmp_Consultation/\"\n",
    "(df_consult.coalesce(1)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(tmp_path_consult))\n",
    "\n",
    "# Récupérer le fichier part-xxxxx.csv\n",
    "part_file_consult = [f.path for f in dbutils.fs.ls(tmp_path_consult) if f.name.startswith(\"part-\")][0]\n",
    "final_path_consult = gold_adls + \"Consultation.csv\"\n",
    "dbutils.fs.mv(part_file_consult, final_path_consult)\n",
    "dbutils.fs.rm(tmp_path_consult, recurse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca0aa328-be09-4691-9a82-c993ddf99360",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Agrégations pour Patient.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42619860-d230-43a7-9373-462c630136e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, when, col\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Charger le fichier Patient.csv\n",
    "df_patient = spark.read.option(\"header\", \"true\").csv(silver_adls + \"Patient.csv\")\n",
    "\n",
    "# ➤ Répartition par sexe\n",
    "agg_sexe = df_patient.groupBy(\"Gender\").count()\n",
    "\n",
    "# ➤ Répartition par tranche d’âge\n",
    "df_patient = df_patient.withColumn(\"AnneeNaissance\", year(\"Birth Date\").cast(\"int\"))\n",
    "df_patient = df_patient.withColumn(\"TrancheAge\", \n",
    "    when(col(\"AnneeNaissance\") >= 2006, \"0-18\")\n",
    "    .when(col(\"AnneeNaissance\") >= 1989, \"19-35\")\n",
    "    .when(col(\"AnneeNaissance\") >= 1964, \"36-60\")\n",
    "    .otherwise(\"60+\"))\n",
    "\n",
    "agg_age = df_patient.groupBy(\"TrancheAge\").count()\n",
    "\n",
    "# ➤ Répartition géographique (nationalité)\n",
    "agg_nationalite = df_patient.groupBy(\"Nationality\").count().orderBy(\"count\", ascending=False)\n",
    "\n",
    "# Chemin vers le dossier d'agrégations dans le container Gold\n",
    "agg_path = gold_adls + \"agregations/\"\n",
    "\n",
    "# === Sauvegarder les résultats un par un ===\n",
    "def save_aggregation(df, name):\n",
    "    tmp_path = agg_path + \"_tmp_\" + name + \"/\"\n",
    "    final_path = agg_path + name + \".csv\"\n",
    "    \n",
    "    (df.coalesce(1)\n",
    "       .write\n",
    "       .mode(\"overwrite\")\n",
    "       .option(\"header\", \"true\")\n",
    "       .csv(tmp_path))\n",
    "    \n",
    "    part_file = [f.path for f in dbutils.fs.ls(tmp_path) if f.name.startswith(\"part-\")][0]\n",
    "    dbutils.fs.mv(part_file, final_path)\n",
    "    dbutils.fs.rm(tmp_path, recurse=True)\n",
    "\n",
    "# Sauvegardes\n",
    "save_aggregation(agg_sexe, \"repartition_par_sexe\")\n",
    "save_aggregation(agg_age, \"repartition_par_tranche_age\")\n",
    "save_aggregation(agg_nationalite, \"repartition_par_nationalite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17c0f2b7-7c48-42d6-81e9-4289108aff69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Agrégations sur Cout.csv et CoutTraitement.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "383bf5eb-21e1-40e5-9c37-438982fb3f3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Lire les deux fichiers CSV\n",
    "df_patient = spark.read.option(\"header\", \"true\").csv(silver_adls + \"Patient.csv\")\n",
    "df_consultation = spark.read.option(\"header\", \"true\").csv(silver_adls + \"Consultation.csv\")\n",
    "\n",
    "# Effectuer la jointure interne sur Encontre_ID\n",
    "df_jointure = df_patient.join(\n",
    "    df_consultation,\n",
    "    on=\"Patient_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b85dee11-1a4c-4944-bd4c-933a57e57b3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Row(Patient_ID='TR754', First Name='Xu', Last Name='Chen', Gender='Male', Birth Date='1922-04-06', Height='173', Weight='62', Marital Status='Divorced', Nationality='Chinese', Blood Type='A+', Encounter_ID='2433', Disease_ID='1490', ResponsibleDoctorID='856', InsuranceKey='82', RoomKey='203', CheckinDate='2024-06-05 21:24:00', CheckoutDate='2024-07-02 20:35:00', CheckinDateKey='20240605', CheckoutDateKey='20240702', Patient_Severity_Score='68.4', RadiologyType='None', RadiologyProcedureCount='0', EndoscopyType='None', EndoscopyProcedureCount='0', CompanionPresent='True')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jointure.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5367cfc2-a5da-4aa3-bb65-a5fda19777d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Lire les deux fichiers CSV\n",
    "df_Cout = spark.read.option(\"header\", \"true\").csv(silver_adls + \"Cout.csv\")\n",
    "\n",
    "# Effectuer la jointure interne sur Encontre_ID\n",
    "df_jointure_2= df_jointure.join(\n",
    "    df_Cout,\n",
    "    on=\"Encounter_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1de01940-da60-4ec1-9302-feb2071c05ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Row(Encounter_ID='5925', Patient_ID='TRDTW246', First Name='Mei Jia Zi', Last Name='Tai Tian', Gender='Female', Birth Date='1968-07-03', Height='131', Weight='51', Marital Status='Married', Nationality='Japanese', Blood Type='O-', Disease_ID='1449', ResponsibleDoctorID='844', InsuranceKey='82', RoomKey='206', CheckinDate='2024-03-06 12:41:00', CheckoutDate='2024-04-17 11:58:00', CheckinDateKey='20240306', CheckoutDateKey='20240417', Patient_Severity_Score='2.2', RadiologyType='Ultrasound', RadiologyProcedureCount='1', EndoscopyType='None', EndoscopyProcedureCount='0', CompanionPresent='True', CostType='NutritionCost', CostAmount='4410.0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jointure_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b8a8d58-edd9-470e-ba57-82d716e88330",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Lire les deux fichiers CSV\n",
    "df_Traitement = spark.read.option(\"header\", \"true\").csv(silver_adls + \"Traitement.csv\")\n",
    "\n",
    "# Effectuer la jointure interne sur Encontre_ID\n",
    "df_jointure_3= df_jointure_2.join(\n",
    "    df_Traitement,\n",
    "    on=\"Encounter_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca4129a2-b2b9-4501-a3f0-23ca99211c33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Row(Encounter_ID='5925', Patient_ID='TRDTW246', First Name='Mei Jia Zi', Last Name='Tai Tian', Gender='Female', Birth Date='1968-07-03', Height='131', Weight='51', Marital Status='Married', Nationality='Japanese', Blood Type='O-', Disease_ID='1449', ResponsibleDoctorID='844', InsuranceKey='82', RoomKey='206', CheckinDate='2024-03-06 12:41:00', CheckoutDate='2024-04-17 11:58:00', CheckinDateKey='20240306', CheckoutDateKey='20240417', Patient_Severity_Score='2.2', RadiologyType='Ultrasound', RadiologyProcedureCount='1', EndoscopyType='None', EndoscopyProcedureCount='0', CompanionPresent='True', CostType='NutritionCost', CostAmount='4410.0', Treatment_ID='3398', Treatment_Type='Therapy', Treatment_Name='Dialysis', Follow_Up='Yes', Complications='No', Drug_Boxes_Used='0', Therapy_Sessions='75', Drug_Cost='0', Surgery_Cost='0', Post_Surgery_Care_Cost='0', Education_Rehab_Cost='75000', Hospital_Drug_Quantity='0', Discharge_Drug_Quantity='0', Total_Drug_Quantity='0')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jointure_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f21b85f-11f3-40db-a996-e444bc1fd4de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, when, col\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ➤ Coût total par patient\n",
    "\n",
    "agg_couttotal = df_jointure_3.groupBy(\"Patient_ID\").agg(F.sum(\"CostAmount\").alias(\"Montant\"))\n",
    "\n",
    "# ➤ Coût moyen par consultation\n",
    "agg_coutmoyerparconsultation = df_jointure_3.groupBy(\"Encounter_ID\").agg(F.avg(\"CostAmount\").alias(\"CoutMoyen\"))\n",
    "\n",
    "# ➤ Coût total des traitements\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_jointure_3 = df_jointure_3.withColumn(\n",
    "    \"Cout_Total_Par_Encontre\",\n",
    "    col(\"Drug_Cost\").cast(\"double\") +\n",
    "    col(\"Surgery_Cost\").cast(\"double\") +\n",
    "    col(\"Post_Surgery_Care_Cost\").cast(\"double\") +\n",
    "    col(\"Education_Rehab_Cost\").cast(\"double\")\n",
    ")\n",
    "\n",
    "agg_couttotaltraitement = df_jointure_3.groupBy(\"Treatment_ID\") \\\n",
    "    .agg(F.sum(\"Cout_Total_Par_Encontre\").alias(\"CoutTotal\")) \\\n",
    "    \n",
    "# Chemin vers le dossier d'agrégations dans le container Gold\n",
    "agg_path = gold_adls + \"agregations/\"\n",
    "\n",
    "# === Sauvegarder les résultats un par un ===\n",
    "def save_aggregation(df, name):\n",
    "    tmp_path = agg_path + \"_tmp_\" + name + \"/\"\n",
    "    final_path = agg_path + name + \".csv\"\n",
    "    \n",
    "    (df.coalesce(1)\n",
    "       .write\n",
    "       .mode(\"overwrite\")\n",
    "       .option(\"header\", \"true\")\n",
    "       .csv(tmp_path))\n",
    "    \n",
    "    part_file = [f.path for f in dbutils.fs.ls(tmp_path) if f.name.startswith(\"part-\")][0]\n",
    "    dbutils.fs.mv(part_file, final_path)\n",
    "    dbutils.fs.rm(tmp_path, recurse=True)\n",
    "\n",
    "# Sauvegardes\n",
    "save_aggregation(agg_couttotal, \" Coût total par patient\")\n",
    "save_aggregation(agg_coutmoyerparconsultation, \"Coût moyen par consultation\")\n",
    "save_aggregation(agg_couttotaltraitement, \"Coût total des traitements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09452507-ad48-4d08-ad89-14c8f58b2aff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tous les fichiers (sauf Consultation.csv et Date.csv) ont été copiés dans le container Gold.\n"
     ]
    }
   ],
   "source": [
    "# Liste des fichiers à exclure\n",
    "exclude_files = {\"Consultation.csv\", \"Date.csv\"}\n",
    "\n",
    "# Liste tous les fichiers CSV dans le container Silver\n",
    "silver_files = [f for f in dbutils.fs.ls(silver_adls) if f.name.endswith(\".csv\") and f.name not in exclude_files]\n",
    "\n",
    "for file in silver_files:\n",
    "    filename = file.name.replace(\".csv\", \"\")\n",
    "    tmp_input_path = silver_adls + filename + \".csv\"\n",
    "\n",
    "    # Lire le fichier depuis Silver\n",
    "    df = spark.read.option(\"header\", \"true\").csv(tmp_input_path)\n",
    "\n",
    "    # Écrire dans un dossier temporaire sous Gold\n",
    "    tmp_output_path = gold_adls + \"_tmp_\" + filename + \"/\"\n",
    "    (df.coalesce(1)\n",
    "       .write\n",
    "       .mode(\"overwrite\")\n",
    "       .option(\"header\", \"true\")\n",
    "       .csv(tmp_output_path))\n",
    "\n",
    "    # Récupérer le nom du fichier part-xxxxx.csv\n",
    "    tmp_files = dbutils.fs.ls(tmp_output_path)\n",
    "    part_file = [f.path for f in tmp_files if f.name.startswith(\"part-\")][0]\n",
    "\n",
    "    # Définir le chemin final dans Gold\n",
    "    final_output_path = gold_adls + filename + \".csv\"\n",
    "\n",
    "    # Déplacer le fichier final et supprimer le dossier temporaire\n",
    "    dbutils.fs.mv(part_file, final_output_path)\n",
    "    dbutils.fs.rm(tmp_output_path, recurse=True)\n",
    "\n",
    "print(\"✅ Tous les fichiers (sauf Consultation.csv et Date.csv) ont été copiés dans le container Gold.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "gold_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
